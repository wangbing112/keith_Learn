# keith_Learn
![image](https://github.com/wangbing112/keith_Learn/assets/78155747/aef99234-4987-48f5-97ff-e102b4e20683)

- [0.介绍](#0.介绍)

# 0. 介绍
## 0.1 大模型开发范式
![image](https://github.com/wangbing112/keith_Learn/assets/78155747/6a8791f5-309a-46cb-8e6c-aeff1dc8a4c4)

目前的核心大模型应用开发方案是InternLM的通用大模型，具有强大的语言理解、指令跟随和语言生成的能力。他们可以理解用户自然语言的指令，具有强大的知识储备和一定的逻辑推理能力，能够作为基座模型知识多元应用是解决复杂问题和应用于多领域的强大工具。
然而当下的大模型也存在一定的局限性，这限制了大模型投入真正的落地应用。例如，大模型的知识时效性受限，大模型是在确定的时间点训练，会使用到大量的训练语料，因此具有强大的知识储备，但这些知识仅包含训练时间点之前的知识，对于更新的知识模型是不具有也无法做出回答的。
例如，对于在2023年训练的模型，无法回答2024年1月1日发生了什么重要事件，而大模型高昂的训练成本也是通过多次训练来保证知识时效性无法实现。因此，如何让大模型能够获取到最新的知识，是将大模型投入实际生产应用的一个重要问题。

其次，通用大模型的专业能力有限。以InternLM为例的通用大模型为例，他们可以在多个领域上解答简单的问题，具有较强的知识广度。但对于垂直领域的专业问题，针对一些需要专业知识复杂底蕴才能解决的问题，通用大模型往往捉襟见肘。
但是在垂直领域如医学、法学等专业性较强的领域，能够表现良好的模型才具有最广阔的应用前景和发展潜力。因此，如何打造垂直大模型是大模型发展的一个重要方向。同时，大模型高昂的训练成本使其定制化成本极高，但是通用的非定制化的应用解决用户需求的能力又往往受限。因此如何打造个人专属的大模型应用，也是大模型应用的一个重要问题。

![image](https://github.com/wangbing112/keith_Learn/assets/78155747/0628f844-f0f6-4c54-9acc-d7f0ba407583)

针对上述大模型的局限性，目前有两种核心的大模型开发范式，通过不同的思路来拓展大模型能力，解决其面临的局限性。
这两种开发方式分别是检索增强生成，也就是我们俗称的RAG，以及传统自然语言处理算法的微调，也就是Finetune。
而RAG的核心思想是给大模型外挂一个知识库。对于用户的提问会首先从知识库中匹配到提问对应回答的相关文档，然后将文档和提问一起交给大模型来生成回答，从而提高大模型的知识储备。
而Finetune的核心思想是在一个新的较小的训练集上进行轻量级的训练微调，从而提升模型在这个新数据集上的能力。

这两种开发方式都能够突破通用大模型的自身局限，但也存在不同的优劣势。
首先对于RAG其核心优势在于成本低且可实时更新。
RAG范式的应用无需对大模型进行重新训练，不需要GPU算力。
对于新的知识只需组织加入到外挂知识库中即可。加入新知识成本极低，可以实时更新，
但且能力受基座模型影响大，基座模型的能力上限极大程度决定了RAG应用的能力天花板。
同时RAG应用每次需要将检索导入相关文档，和用户提问一起交给大模型进行回答，占用了大量的模型上下文，因此回答知识有限。对于一些需要大跨度收集知识进行总结性回答的问题表现不佳。

而对于Finetune，其核心优势在于可个性化微调，且知识覆盖面广。
Finetune范式的应用将在个性化数据上微调，因此可以充分的和个性化数据，尤其是对于非可见知识，如回答风格模拟效果非常好。
同时Finetune范式的应用是一个新的个性化大模型，其仍然具有大模型的广阔知识域，因此可以回答的问题知识覆盖面广。
但是他的应用需要在新的数据集上进行训练，成本高昂，需要很多的GPU算力和个性化数据。
同时Finetune是无法解决实时更新的问题，因为其更新的成本仍然很高。

![image](https://github.com/wangbing112/keith_Learn/assets/78155747/2b315a03-b91e-4e58-8480-6dcbaed872dc)

首先简单介绍RAG的原理。正如图中所示，对于每一个用户输入，我们的应用会首先将基于向量模型sentence transformer将输入文本转化为向量，并在向量数据库中匹配相似的文本段。
在这里我们认为与问题相似的文本段大概率包含了问题的答案。
然后我们会将用户的输入和检索到的相似文本段一起嵌入到模型的prompt中，传递给intelnLM，要求他对问题做出最终的回答，作为最后的输出，这就是RAG的基本思想。

## 0.2 LangChain简介

![image](https://github.com/wangbing112/keith_Learn/assets/78155747/d34f0d41-0469-4010-b97b-64dc0b8bcfc4)

那么我们如何可以快速高效的开发一个RA级应用开源框架？LangChain为我们提供了这样的可能性。
LangChain是一个针对大模型开发的第三方开源框架，旨在通过为各种大模型提供通用接口，来简化基于大模型的应用程序开发流程，从而帮助开发者自由构建大模型应用。
当这样封装了很多组件，通过这些组件的组合可以构建多种类型的RAG应用。
开发者可以直接将私人数据去嵌入到LangChain中的组件，然后通过对这些组件进行组合来构建适用于自己业务场景的应用。

当前最主要也最核心的组成模块是链，也就是Chain，一个链就是将多个组件组合在一起的端到端应用，也就是通过一个链可以封装一系列的大模型的操作。
例如最经典的链接为检索问答链，也就是我们将在本节课使用到的组件。检索问答链通过将大模型、向量数据库等多个组件组合在一起覆盖，实现了RAG的全部流程。
我们可以直接将我们的数据库和大模型引入到检索问答链中，从而高效的搭建RAG应用。

![image](https://github.com/wangbing112/keith_Learn/assets/78155747/98af974e-cbf5-4b0f-9dd3-5fb65b001174)

这张图展示了我们如何基于LangChain来搭建RAG应用。首先对于以本地文档形式存在的个人知识库，会使用Unstructed loader组件来加载本地文档。这个组件会将不同格式的本地文档统一转换为纯文本格式，
然后通过text splitter组件对提取出来的纯文本进行分割成Chunk，再通过开源词向量模型sentence transformer来将文本段转化为向量格式，存储到基于Chroma的相对应数据库中。
接下来对于用户的每一个输入，会首先通过sentence transformer将输入转化为同样维度的向量。
通过在向量数据库中进行相似度的匹配，找到和用户输入相关的文本段，
将相关的文本段嵌入到已经写好的prompt template中，再交给InternLM进行最后的回答即可。
上述这一整个过程都会被封装在检索问答中，我们可以直接将个性化的配置引入到检索问答的对象，即可构建属于自己的RAG应用。

## 0.3 构建向量数据库

![image](https://github.com/wangbing112/keith_Learn/assets/78155747/fcde0e1f-722b-47cb-822a-732747e5c3e5)

接下来我们将深入基于LangChain搭建RAG应用的基本步骤，讲解我们如何一步步搭建自己的知识库。
首先我们需要基于个人数据构建向量数据库。向量数据库的构建主要有加载源文档、文档分块和文档向量化三个部分。
由于我们的个人数据可能有多种数据类型，例如TXT、mark down或者PDF等。
我们首先需要确定原文档的类型，针对不同的类型的源文件选用不同的加载器。这些加载器的核心功能都是带格式的文本转化为无格式的字符串。
我们后续构造向量数据库的操作中，输入都是无格式的纯文本。

然后由于大模型的输入上下文往往都是有限的，单个文档的长度往往会超过模型上下文的上限。
我们需要对加载的文本进行切分，将它划分到多个不同的trunk，后续检索相关的trunk来实现问答，
文本分割一般按字符串长度进行分割，例如设定最长的字符串长度为500，那么每500个字符会被切分为一个trunk。
我们还可以手动控制分割块的长度和重叠区域的长度，从而提升我们向量数据库的检索效果。

最后为了使用向量数据库来支持语义检索，也就是我们想要输入问题检索到相关的答案，我们需要将文档向量化存入向量数据库。
我们可以使用任意一种embedding模型来进行向量化。在本节课后续的实战环节，我们会使用开源词向量sentence transformer来进行向量化，
同时我们也可以使用多种支持语音检索的向量数据库。我们一般使用轻量级适合入门的chroma数据库来搭建。

## 0.4 搭建知识库助手

![image](https://github.com/wangbing112/keith_Learn/assets/78155747/be19c6b2-245d-4bf8-97e4-975e83ddcc22)

在完成向量数据库的构建之后，我们就可以基于InternLM模型来搭建我们的知识库助手，
LangChain支持多种LLM，也支持自定义的LLM。我们可以将InternLM部署在本地，将它封装成一个LangChain自定义LMM类，
在这个类中，调用本地部署的InternLM来将InternLM接入到LangChain框架中，从而高效使用LangChain的检索问答链组件，

![image](https://github.com/wangbing112/keith_Learn/assets/78155747/134c51e3-dd65-4388-b53d-493f07ab7691)

LangChain的检索问答组件，可以自动实现知识检索、prompt的嵌入、大模型回答的全部流程。
也就是说，我们可以将基于InternLM的自定义LLM和构建的向量数据库直接接入到检索问答链的上游。
然后调用检索问答链，它会自动完成对用户输入进行向量化，在向量数据库中检索相关文档片段，基于InternLM的自定义大模型进行问题回答的全部过程。
要用这样一个检索文档链就可以实现我们知识库助手的核心功能。

![image](https://github.com/wangbing112/keith_Learn/assets/78155747/772096b3-725a-45ee-b27a-f0c0d5ffca31)

其实问答的性能还是有所局限，基于RAG的问答系统性能核心受限于两个因素，一个是检索精度，一个是prompt的性能。
在这里我们提出一些可能的优化建议。如果你搭建起了最简单的知识库助手，还想进一步提升知识库助手的回答性能，可以参考这些建议进行优化。
从检索方面的出发，我们可以考虑基于语义而不是字符长度来进行trunk的切分，从而保证每一个trunk的语义完整性。由于我们的检索是以trunk为单位进行检索的，如果部分trunk的语义不完整，那么模型可能会丢失一些重要信息。
同时我们可以考虑给每一个trunk生成概括性的索引，在检索时匹配索引而不是全文，来找到相关文档片段，从而提高检索召回的精度。
而从prompt的方面，我们可以迭代优化prompt的策略，来不断激发模型的潜在能力，提高回答的性能。


## 0.5 web_demo部署

![image](https://github.com/wangbing112/keith_Learn/assets/78155747/d196e4ec-9efd-48f2-93a7-f64bafc2fe90)









